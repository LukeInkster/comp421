{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second COMP421 Assignment\n",
    "\n",
    "Suggestion: just edit and add cells to this, and submit a notebook? Give it a name that _starts with your name_, such as Frean_SecondAssignment.ipynb\n",
    "\n",
    "Please submit using the usual ECS [submission system](https://apps.ecs.vuw.ac.nz/submit/COMP421).\n",
    "\n",
    "### What this is about\n",
    "\"Autoencoders\" are neural networks which attempt to reproduce their input at their output, after having passed the information through some form of bottleneck, such as a hidden layer having fewer units than the input. They are trained in the usual way (in this case, by doing gradient descent on the sum of squared errors), so in a sense they're just regular backprop networks. What makes them interesting is the _encoding_ of the inputs that results. The mapping from input to the bottleneck layer can be thought of as an \"encoder\" and, from there to the output as a \"decoder\". To the extent that the network succeeds in reproducing the input, it has captured the essential structure in the encoding, ie. the hidden layer activations.\n",
    "\n",
    "So when you learn an autoencoder, using the classic supervised learning algorithm of backpropagation, you're really doing a form of _unsupervised_ learning: in the learning process the network is \"discovering\" a compact encoding of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREP: get comfortable with autograd\n",
    " \n",
    "autograd is probably the best thing since sliced bread. You write some function, then autograd will give you a function that takes the gradient of it. Here is an example: [autograd on sin function](https://github.com/garibaldu/comp421/blob/master/notebooks/autograd_example.ipynb). For part B here you will write your own neural network's forward pass and loss function, and use autograd to find the gradient for use in Stochastic Gradient Descent. \n",
    "\n",
    "(No more messing about with awkward buggy gradient calculations! No more tearing that loaf apart with your bare hands!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Principal Components Analysis\n",
    "\n",
    "Find the first few Principal Components of the sklearn digits data (ie. the data from  the [kNN example](https://github.com/garibaldu/comp421/blob/master/notebooks/knn_on_little_digits.ipynb)), and show them as images. It doesn't really matter how you do this - perhaps sklearn itself can do it for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: autoencoding with linear hidden layer\n",
    "   1. use autograd to learn a neural network that is entirely linear, with inputs _and outputs_ being the sklearn digits, and a single hidden unit. Show the input and output weights: are they the same as the first principal component?\n",
    "   1. now freeze that unit's weights (both in and out) and learn a second hidden unit.  Show the input and output weights: are they the same as the second principal component?\n",
    "   1. now train the whole thing at once, instead of unit-by-unit. Are the results different?\n",
    "   1. account for what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: the effect of nonlinearities\n",
    "   1. now do the same (training all at once) with a hidden layer that involves a non-linearity (e.g. sigmoid, or relu).\n",
    "   1. account for what you see."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
