# Week 11 - Variational Autoencoders


## beyond vanilla RNNs: LSTM and GRU variants
`(Tuesday: Marcus)`
* Motivated by the vanishing/exploding gradients problem of vanilla RNNs, LSTM and GRU (and others) are architectures which are able to learn richer structure in time.
* we talked through the post that picks apart LTSM in particular on [Colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs)
* and we looked at the GRU architecture too

## Variational autoencoders
`(Wednesday: Luke and Harry)`
* we looked at a [Tutorial on Variational Autoencoders](http://arxiv.org/abs/1606.05908)
* [slides](variational_autoencoders.pdf)
* we will continue this discussion (and get to the the main paper [Autoencoding Variational Bayes](http://arxiv.org/abs/1606.05908)) next Tuesday.

## Catch-up: HMMs vs Kalman Filters vs Particle Filters
`(Friday: Marcus)`
* (we're doing this now rather than a straight continuation of VAE stuff, as Max is away)

