{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horsing around with the backprop algorithm\n",
    "Marcus started this see how quickly he could get backprop to stand up.\n",
    "\n",
    "Note the use of \"checkgrad\", which exhaustively confirms that the gradient calculation is in fact correct - not something to run all the time but a useful check to have.\n",
    "\n",
    "Issues:\n",
    "  * the neural net has no biases yet\n",
    "  * the learning problem is just random - better if we could read in a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import sklearn\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.cross_validation as cv\n",
    "import sklearn.neighbors as nb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision = 2, suppress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify a neuron transfer function ('f'), and its derivative ('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THESE FUNCTIONS MUST MATCH ONE ANOTHER................\n",
    "\n",
    "def f( phi ):  \n",
    "    # phi is always going to be a weighted sum (probably a matrix of).\n",
    "    x = 1.0/ (1.0 + np.exp(-phi))\n",
    "    \n",
    "    #ALT: rectified linear goes like this\n",
    "    #x = phi * (phi>0.0)\n",
    "    return x\n",
    "\n",
    "def df( x ):  # MUST MATCH WHAT YOU PUT HERE with the f function.\n",
    "    # This is the gradient of the transfer function\n",
    "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
    "    # HOWEVER NOTE that unlike f(), the input argument isn't phi here - it's the function value itself.\n",
    "\n",
    "    dx = x*(1-x)\n",
    "    #ALT: rectified linear goes like this\n",
    "    #dx = 1.0*(x>0.0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get or make some training data\n",
    "Got to have something to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02 -0.55  0.96 -1.23 -0.52  1.01  1.99 -0.35]\n",
      " [-1.47 -1.89 -0.42  0.05 -1.12 -1.35 -0.35  0.28]\n",
      " [ 0.71  0.37  0.04  0.28  1.57 -0.41 -0.88  1.03]]\n",
      "[[ 0.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
    "# However you do it, call the input patterns \"inpats\" (each row is a pattern), and the output patterns \"targets\".\n",
    "Nins, Nouts, Npats = 8, 2, 100\n",
    "inputX = rng.normal(0,1,size=(Npats,Nins))\n",
    "tmpNhids = 10\n",
    "tmp_weights = rng.normal(0,1,size=(Nins,tmpNhids))\n",
    "hidphi = np.dot(inputX, tmp_weights)\n",
    "tmp_weights = rng.normal(0,1,size=(tmpNhids,Nouts))\n",
    "phi = np.dot(f(hidphi), tmp_weights)\n",
    "Targ = 1.0* (f(phi) > 0.5)\n",
    "print (inputX[:3, :])\n",
    "print (Targ[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It would be better to use some realistic data actually.\n",
    "# But this is a classification dataset, so we'll need more than 1 transfer function \n",
    "# (one for the hidden layers, but a softmax for the output). So I won't cloud the issue here.........\n",
    "\n",
    "#digits = ds.load_digits()\n",
    "#X = digits.data\n",
    "#Targ = digits.target  # currently a vector - but we want a nx1 matrix,so...\n",
    "#Targ = np.reshape(Targ, (len(Targ), 1))  \n",
    "#print(X.shape, Targ.shape)\n",
    "#print(X.min(), X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function we're climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_goodness(Y, Targ):  # often this is called the \"Loss\" or the \"Cost function\" (and given a minus sign accordingly).\n",
    "    # outputs is a matrix (Npats, Nouts), and targets is what we'd like those to be.\n",
    "    difference = Targ - Y\n",
    "    Goodness_vec = -0.5*np.power(difference,2.0) # inverted parabola centered on the target outputs\n",
    "    dGoodness_vec = Targ - Y # Bit of a fluke: for this loss fn, it's the same as the difference (not always true). \n",
    "    return Goodness_vec.sum(), Goodness_vec, dGoodness_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the network's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many neurons in each layer:  [8, 5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "Npats = inputX.shape[0]\n",
    "architecture = [inputX.shape[1], 5, 3, Targ.shape[1]]\n",
    "#architecture = [inpats.shape[1], targets.shape[1]]\n",
    "print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 activations have shape  (100, 8)\n",
      "layer 1 activations have shape  (100, 5)\n",
      "layer 2 activations have shape  (100, 3)\n",
      "layer 3 activations have shape  (100, 2)\n"
     ]
    }
   ],
   "source": [
    "X = [inputX] \n",
    "# X is going to be a list giving the activations of successive layers. \n",
    "# Each one is a matrix, whose columns are the neurons in that layer.\n",
    "# Each row in the matrix corresponds to a training item.\n",
    "# So all the matrices in the list X will have the same number of rows.\n",
    "\n",
    "for L in range(1, len(architecture)):\n",
    "    X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
    "\n",
    "for L in range(len(architecture)): \n",
    "    print('layer %d activations have shape ' %(L), X[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 weights have shape  ()\n",
      "layer 1 weights have shape  (5, 8)\n",
      "layer 2 weights have shape  (3, 5)\n",
      "layer 3 weights have shape  (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Then we have the weights. I'm going to index weight layer by the layer they're *going* towards.\n",
    "# So I'll have a zeroth weight layer for sanity, but it's going to be empty!\n",
    "# NOTE: no implementation of bias weights here, yet!\n",
    "W  = [np.array(None)]\n",
    "dW = [np.array(None)]\n",
    "for L in range(1,len(X)):\n",
    "    init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
    "\n",
    "    Nins = X[L-1].shape[1]\n",
    "    Nouts = X[L].shape[1]\n",
    "    W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
    "    dW.append(0.0 * np.copy(W[L]))\n",
    "\n",
    "for L in range(len(W)):\n",
    "    print('layer %d weights have shape ' %(L), W[L].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W):\n",
    "    for L in range(1,len(X)):\n",
    "        x = X[L-1].transpose()\n",
    "        # print (L, W[L].shape, x.shape)\n",
    "        X[L] = f(np.dot(W[L], x).transpose())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(X, W, dW, targets):\n",
    "    good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "    epsilon = dgood\n",
    "    npats = X[0].shape[0]\n",
    "    for L in range(len(X)-1,0,-1):\n",
    "        psi = epsilon * df(X[L]) # elt-wise multiply\n",
    "        n1, n2 = X[L-1].shape[1], psi.shape[1]\n",
    "        A = np.tile(X[L-1],n2).reshape(npats,n2,n1)\n",
    "        B = np.repeat(psi,n1).reshape(npats,n2,n1)        \n",
    "        dW[L] = (A*B).sum(0) # outer product multiply\n",
    "        epsilon = np.dot(psi, W[L]) # inner product multiply\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = forward_pass(X, W)\n",
    "dW = backward_pass(X, W, dW, Targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkgrad(dW, X, W, targets):\n",
    "    # Check the gradient directly, via perturbations to every weight.\n",
    "    # This is completely daft in practical terms, but very useful for debugging.\n",
    "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
    "    tiny = 0.0001\n",
    "    \n",
    "    dW_test = [np.array(None)]\n",
    "    for L in range(1,len(W)):\n",
    "        dW_test.append(0.0*np.copy(W[L]))\n",
    "    \n",
    "    X = forward_pass(X,W)\n",
    "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "    \n",
    "    for L in range(1,len(X)):\n",
    "        for j in range(W[L].shape[0]): # index of destination node\n",
    "            for i in range(W[L].shape[1]): # index of origin node\n",
    "                # perturb that weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
    "                # compute and store the empirical gradient estimate\n",
    "                X = forward_pass(X,W)\n",
    "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets)\n",
    "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny                \n",
    "                # unperturb the weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
    "                \n",
    "    # show the result?\n",
    "    for L in range(1,len(X)):\n",
    "        print ('-------------- layer %d --------------' %(L))\n",
    "        print ('calculated gradients:')\n",
    "        print (dW[L])\n",
    "        print ('empirical gradients:')\n",
    "        print (dW_test[L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- layer 1 --------------\n",
      "calculated gradients:\n",
      "[[ 0.    0.    0.   -0.    0.    0.   -0.   -0.  ]\n",
      " [ 0.01  0.    0.   -0.    0.   -0.   -0.    0.01]\n",
      " [ 0.    0.    0.   -0.    0.    0.   -0.   -0.  ]\n",
      " [-0.01 -0.01 -0.    0.01 -0.   -0.    0.01 -0.01]\n",
      " [ 0.    0.    0.   -0.    0.    0.   -0.    0.  ]]\n",
      "empirical gradients:\n",
      "[[ 0.    0.    0.   -0.    0.    0.   -0.   -0.  ]\n",
      " [ 0.01  0.    0.   -0.    0.   -0.   -0.    0.01]\n",
      " [ 0.    0.    0.   -0.    0.    0.   -0.   -0.  ]\n",
      " [-0.01 -0.01 -0.    0.01 -0.   -0.    0.01 -0.01]\n",
      " [ 0.    0.    0.   -0.    0.    0.   -0.    0.  ]]\n",
      "-------------- layer 2 --------------\n",
      "calculated gradients:\n",
      "[[-0.14 -0.14 -0.14 -0.14 -0.14]\n",
      " [ 0.26  0.26  0.25  0.25  0.25]\n",
      " [-0.12 -0.13 -0.13 -0.14 -0.13]]\n",
      "empirical gradients:\n",
      "[[-0.14 -0.14 -0.14 -0.14 -0.14]\n",
      " [ 0.26  0.26  0.25  0.25  0.25]\n",
      " [-0.12 -0.13 -0.13 -0.14 -0.13]]\n",
      "-------------- layer 3 --------------\n",
      "calculated gradients:\n",
      "[[-5.44 -5.88 -5.65]\n",
      " [-0.99 -1.05 -1.03]]\n",
      "empirical gradients:\n",
      "[[-5.44 -5.88 -5.65]\n",
      " [-0.99 -1.05 -1.03]]\n"
     ]
    }
   ],
   "source": [
    "checkgrad(dW, X, W, Targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yay.\n",
    "The gradient seems to be right for the full MLP, so that's... progress!\n",
    "\n",
    "Let's try learning the problem then...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(X, W, dW, targets, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
    "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
    "    times, vals = [], []\n",
    "    next_time = 0\n",
    "    \n",
    "    prev_change = [np.array(None)]\n",
    "    for L in range(1,len(X)):\n",
    "        prev_change.append(0.0 * np.copy(W[L]))\n",
    "    \n",
    "    # now for the learning iterations\n",
    "    for step in range(num_steps):\n",
    "        X = forward_pass(X,W)\n",
    "        \n",
    "        # this is just record-keeping.......\n",
    "        if step == next_time:\n",
    "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets)\n",
    "            vals.append(good_sum)\n",
    "            times.append(step)\n",
    "            next_time = step + 10\n",
    "\n",
    "        dW = backward_pass(X, W, dW, targets)\n",
    "        for L in range(1,len(X)):\n",
    "            change =  (learning_rate * dW[L])  +  (momentum * prev_change[L])\n",
    "            W[L] = W[L] + change\n",
    "            prev_change[L] = change\n",
    "\n",
    "\n",
    "    return W, times, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x569d838>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x55c5a88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, vals, times = learn(X, W, dW, Targ, learning_rate=0.01, momentum=0.5, num_steps=10000)\n",
    "plt.plot(vals, times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
