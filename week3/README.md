_Note: explore the ipython notebooks in this directory._

This week is about learning the parameters of a predictor. We'll look at three flavours: polynomials, neural nets, and radial basis functions. We will see that each has its own strengths and weaknesses.

***
# fitting polynomials to data
`(1st lecture, presented by James)`

Today's lecture is a tour of the [first section]() of Chris Bishop's fantastic book [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/). The whole book is well worth getting if you plan to go further with machine learning - see the library for the hardcopy.

 
***

# Back-propagation in neural networks
`(2nd lecture, presented by Jack)`

See also [neural networks and deep learning, Chapter 2](http://neuralnetworksanddeeplearning.com/chap2.html)


***

# A basic hands-on session 
`(3rd lecture slot)`

if you have a laptop, bring it.

Aims:

 * get ipython/jupyter notebook going, and scikit-learn, and numpy, and matplotlib - the simplest way is via [anaconda](https://www.continuum.io/downloads) which (we think) has everything in place.
 * if you get that far, but not everyone has yet please help someone else!
 * interact with the [comp421 repository on github](https://github.com/garibaldu/comp421) - even if you never push back, you should pull it regularly.
 * if you get that far, but not everyone has yet please help someone else!
 * invoke _ipython notebook_ and step through the notebooks on "knn on little digits" (in week1 directory), and "Backprop_marcus" (in week3 directory).
