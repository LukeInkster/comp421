{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the backprop algorithm\n",
    "\n",
    "Note the use of \"checkgrad\", which exhaustively confirms that the gradient calculation is in fact correct - not something to run all the time but a useful check to have.\n",
    "\n",
    "Issues:\n",
    "  * the neural net has no biases yet\n",
    "  * the learning problem is just random - better if we could read in a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import sklearn\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.cross_validation as cv\n",
    "import sklearn.neighbors as nb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=2,suppress=True) #prints floats shorter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify a neuron transfer function ('f'), and its derivative ('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THESE FUNCTIONS MUST MATCH ONE ANOTHER................\n",
    "\n",
    "def f( phi, transfer='linear'): \n",
    "    npats, nunits = phi.shape\n",
    "    # phi is always going to be a weighted sum (probably a matrix of).\n",
    "    if (transfer == 'linear'): return (phi)\n",
    "    elif (transfer == 'sigmoid'): return (1.0/ (1.0 + np.exp(-phi)))\n",
    "    elif  (transfer == 'relu'): return (phi * (phi>0.0))\n",
    "    elif  (transfer == 'softmax'): \n",
    "        # get the max for each row, and reshape so it broadcasts nice.\n",
    "        shift = phi.max(axis=1).reshape((npats,1)) \n",
    "        ePhi = np.exp(phi-shift) # so the max in exp is 0.\n",
    "        Z = ePhi.sum(axis=1).reshape((npats,1))\n",
    "        return (ePhi/Z)\n",
    "    else: return None\n",
    "\n",
    "def df( x, transfer='linear'):  \n",
    "    # MUST MATCH WHAT YOU PUT HERE with the f function.\n",
    "    # This is the gradient of the transfer function\n",
    "    # with respect to \"phi\", the weighted sum of inputs to the neuron.\n",
    "    #\n",
    "    # IMPORTANT: NOTE that unlike f(), the input argument isn't phi \n",
    "    # here - it's the function value itself after going thru non-linearity.\n",
    "\n",
    "    if (transfer == 'linear'): \n",
    "        return (np.ones(shape=x.shape,dtype=float))\n",
    "        # return (1.0)  would probably work just the same!\n",
    "    elif (transfer == 'sigmoid'): return (x*(1.0-x))\n",
    "    elif  (transfer == 'relu'): return (1.0*(x>0.0))\n",
    "    elif  (transfer == 'softmax'): return (x*(1.0-x))\n",
    "    else: return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get or make some training data\n",
    "Got to have something to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input means:  -8.5798035343e-18\n",
      "input variances:  1.0\n"
     ]
    }
   ],
   "source": [
    "# I'm going to be dumb here and make them from my very own random perceptrons!\n",
    "# However you do it, call the input patterns \"inputX\" (each row is a pattern), and the output patterns \"targets\".\n",
    "Nins, Nouts, Npats = 50, 5, 1000\n",
    "inputX = rng.normal(0,1,size=(Npats,Nins))\n",
    "inputX = inputX - inputX.mean(axis=0)  # mean zero\n",
    "inputX = inputX / np.sqrt(inputX.var(axis=0)) # variance one\n",
    "#print (inputX[:3, :]) # just a sanity check\n",
    "print ('input means: ',inputX.mean())\n",
    "print ('input variances: ',inputX.var())\n",
    "Targ = np.zeros((Npats,Nouts))\n",
    "\n",
    "tmpNhids = 10\n",
    "tmp_weights = 1./Nins  * rng.normal(0,1,size=(Nins,tmpNhids))\n",
    "hidphi = np.dot(inputX, tmp_weights)\n",
    "H = f(hidphi,'sigmoid')\n",
    "tmp_weights = 1./np.sqrt(tmpNhids) * rng.normal(0,1,size=(tmpNhids,Nouts))\n",
    "outphi = np.dot(H, tmp_weights)\n",
    "\n",
    "## NOTE sampling of Targ should reflect choice of y transfer fn here:\n",
    "y = f(10.*outphi, 'softmax')\n",
    "for n in range(Npats):\n",
    "    t = np.random.choice(Nouts, 1, p=y[n]) # choose from Categorical\n",
    "    Targ[n,t]=1\n",
    "\n",
    "#print (y[:3, :]) # just a sanity check\n",
    "#print (Targ[:3, :]) # just a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "(1797, 64) (1797, 10)\n",
      "-1.0 1.0\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# That's groovy for testing: you can give the learner net the \n",
    "# same structure and it should \"get there\" reliably.\n",
    "#\n",
    "# But it would be more interesting to use some realistic data too.\n",
    "# This is a classification dataset, so we should have softmax for the output).\n",
    "if True:\n",
    "    digits = ds.load_digits()\n",
    "    inputX = digits.data\n",
    "    Npats = inputX.shape[0]\n",
    "    inputX = inputX - inputX.mean(axis=0)  # mean zero\n",
    "    max_in, min_in = inputX.ravel().max(), inputX.ravel().min() \n",
    "    inputX = 2.0*(inputX - min_in)/(max_in - min_in) - 1.0\n",
    "    print (np.where(inputX.var(axis=0) == 0.0))\n",
    "    #assert np.all(inputX.var(axis=0)), 'some inputs have no variance!'\n",
    "    #inputX = inputX / np.sqrt(inputX.var(axis=0)) # variance one\n",
    "    \n",
    "    target_classes = digits.target  # currently a vector - but we want a nx1 matrix,so...\n",
    "    Nouts = np.max(target_classes)+1\n",
    "    Targ = np.zeros((Npats,Nouts)) \n",
    "    for n in range(Npats):  # There has to be a better way by anyways...\n",
    "        t = target_classes[n]\n",
    "        Targ[n,t]=1\n",
    "    print(inputX.shape, Targ.shape)\n",
    "    print(inputX.min(), inputX.max())\n",
    "    print(Targ[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nins: 64, Nouts: 10, Npats: 1797\n"
     ]
    }
   ],
   "source": [
    "Npats, Nins = inputX.shape\n",
    "Nouts = Targ.shape[1]\n",
    "print('Nins: %d, Nouts: %d, Npats: %d' % (Nins, Nouts, Npats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the network's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialise_net_structure(inputX, architecture, transfer_func):\n",
    "\n",
    "    Nlayers = len(architecture)\n",
    "    assert len(transfer_func) == Nlayers, 'you need a transfer function for every layer'\n",
    "    assert not((Nouts==1) and (transfer_func[-1]=='softmax')), 'softmax of 1 output makes no sense!'\n",
    "    print ('There are this many neurons in each layer: ', architecture)  # a list of the number of neurons in each layer\n",
    "    print ('Transfer of each processing layer: ', transfer_func[1:]) \n",
    "\n",
    "    X = [inputX] \n",
    "    # X is going to be a list giving the activations of successive layers. \n",
    "    # Each one is a matrix, whose columns are the neurons in that layer.\n",
    "    # Each row in the matrix corresponds to a training item.\n",
    "    # So all the matrices in the list X will have the same number of rows.\n",
    "\n",
    "    for L in range(1, len(architecture)):\n",
    "        X.append(np.zeros(shape=(Npats, architecture[L]), dtype=float))\n",
    "\n",
    "    for L in range(len(architecture)): \n",
    "        print('layer %d activations have shape ' %(L), X[L].shape)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many neurons in each layer:  [64, 10, 10]\n",
      "Transfer of each processing layer:  ['sigmoid', 'softmax']\n",
      "layer 0 activations have shape  (1797, 64)\n",
      "layer 1 activations have shape  (1797, 10)\n",
      "layer 2 activations have shape  (1797, 10)\n"
     ]
    }
   ],
   "source": [
    "arch  = [Nins, 10, Nouts]  # here, small and simple as it's a test.\n",
    "trans = ['linear', 'sigmoid', 'softmax'] \n",
    "# NOTE: first is ALWAYS linear, as it's just the input itself.\n",
    "# I don't think I use it anywhere, anyway!\n",
    "X = initialise_net_structure(inputX, arch, trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then we have the weights. I'm going to index the weight layer \n",
    "# according to the layer they're *coming from*.\n",
    "# So I'll have a zeroth weight layer.\n",
    "# NOTE: no implementation of bias weights here, yet!\n",
    "def initialise_weights(X):\n",
    "    W  = [] #[np.array(None)]\n",
    "    for L in range(0,len(X)-1):\n",
    "        init_weights_scale = 0.1  #1/np.sqrt((X[L].shape()).max())\n",
    "\n",
    "        Nins = X[L].shape[1]\n",
    "        Nouts = X[L+1].shape[1]\n",
    "        W.append(init_weights_scale * rng.normal(0,1,size=(Nouts, Nins)) )\n",
    "\n",
    "    for L in range(len(W)):\n",
    "        print('layer %d weights have shape ' %(L), W[L].shape)\n",
    "        \n",
    "    # while we're at it, make space for the gradients so we're not constantly reallocating\n",
    "    dW = []\n",
    "    for w in W:\n",
    "        dW.append(np.zeros(w.shape, dtype=float))\n",
    "        \n",
    "    return W, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 weights have shape  (10, 64)\n",
      "layer 1 weights have shape  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "W, dW = initialise_weights(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function we're climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.0, array([-0.5, -0.5, -0.5, -0.5]), array([1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "def calc_goodness(Y, Targ, output_function='linear'):  \n",
    "    # often this is called the \"Loss\" or the \"Cost function\" \n",
    "    # (and given a minus sign accordingly).\n",
    "    # Y and Targ are matrices of shape (Npats, Nouts).\n",
    "    #\n",
    "    # dGoodness is the gradient w.r.t. the phi of the output layer,\n",
    "    # where phi is the weighted sum into each of those output units.\n",
    "    # GOTCHA: this means that in backprop, we won't need to push the \n",
    "    # gradients through the non-linearity of the output layer: they've \n",
    "    # already done that, here.\n",
    "    if (output_function in ['linear','relu']):\n",
    "        Goodness_vec = -0.5*np.power(Targ - Y, 2.0) # inverted parabola centered on the target outputs\n",
    "        dGoodness_vec = Targ - Y\n",
    "        \n",
    "    elif (output_function == 'sigmoid'): \n",
    "        Goodness_vec = Targ * np.log(Y)  +  (1-Targ) * np.log(1-Y) \n",
    "        dGoodness_vec = Targ - Y\n",
    "\n",
    "    elif (output_function == 'softmax'):  # for softmax though it's going to be...\n",
    "        Goodness_vec = np.multiply(Targ, np.log(Y))\n",
    "        dGoodness_vec = Targ - Y\n",
    "\n",
    "    # TESTING...\n",
    "    #Goodness_vec = np.ones(shape=Targ.shape, dtype=float)\n",
    "    #dGoodness_vec = 0.0*np.ones(shape=Targ.shape, dtype=float)\n",
    "    return Goodness_vec.sum(), Goodness_vec, dGoodness_vec\n",
    "\n",
    "# quick dumb test\n",
    "print(calc_goodness(np.arange(1,5), np.arange(1,5)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass\n",
    "Notation:\n",
    "   * superscript $l$ denotes the layer\n",
    "   * $\\Phi$ is weighted sum (as a vector over all units in a layer)\n",
    "   * $f_l(\\Phi)$ denotes the transfer function in use at layer $l$\n",
    "\n",
    "So the forward pass through a layer can be written as a $\\Phi^{l} \\longrightarrow \\Phi^{l+1}$ function:\n",
    "\n",
    "$$ \\Phi^{l+1} = W^{l} \\cdot f_{l}(\\Phi^{l}) $$\n",
    "where the dot stands for matrix multiplication.\n",
    "\n",
    "It's fun to write this out \"long hand\" like this;\n",
    "\n",
    "$$\\begin{align} \n",
    "\\Phi^{L} &= W^{L-1} \\cdot f_{L-1}(\\Phi^{L-1})  \n",
    "\\\\\n",
    "&= W^{L-1} \\cdot f_{L-1}\\big(W^{L-2} \\cdot f_{L-2}(\\Phi^{L-2}) \\big) \n",
    "\\\\\n",
    "&= W^{L-1} \\cdot f_{L-1}\\big(W^{L-2} \\cdot f_{L-2}\\big(W^{L-3} \\cdot f_{L-3}\\big(\\Phi^{L-3}\\big)\\big) \\big) \n",
    "\\end{align} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W, transfer_fns):\n",
    "    for layer in range(1,len(X)):\n",
    "        phi = np.dot(W[layer-1], X[layer-1].transpose()).transpose()\n",
    "        X[layer] = f(phi, transfer_fns[layer])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward pass\n",
    "Notation:\n",
    "   * here I'll use $\\mathcal{L}$ as the loss. In the code I talk about \"goodness\", but just insert a minus if you want to convert!...\n",
    "   * capital $L$ refers to the final layer, where outputs are generated.\n",
    "   * let's denote the number of units in layer $l$ as $N^l$.\n",
    "   \n",
    "By the chain rule, the whole gradient for some weights in layer $l$ is:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\Phi^l} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\Phi^L}}_\\text{A} \\; \\cdot \\; \\underbrace{\\frac{\\partial \\Phi^{L}}{\\partial \\Phi^{L-1}} \\cdots \\frac{\\partial \\Phi^{l+2}}{\\partial \\Phi^{l+1}}}_\\text{B}  \\; \\cdot \\; \\underbrace{\\frac{\\partial \\Phi^{l+1}}{\\partial W^{l}}}_\\text{C} $$\n",
    "\n",
    "In this \"vectorial\" form this looks nice and compact. Remember each of these is really a matrix though. The numerator's vector elements are rows, the denominator elts are columns of this matrix. So: \n",
    "   * A's shape is $(1 \\times N^L)$. \n",
    "   * B's shapes are $(N^{L} \\times N^{L-1})$, \\;...\\; ,$(N^{l+2} \\times N^{l+1})$.\n",
    "   * C is a tensor (matrix with 3 axes) with shape $(N^{l+1} \\times \\underbrace{N^{l+1} \\times N^{l}}_\\text{shape of $W^l$})$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B has a recursive flavour: the gradient is pushed back through each layer in turn. What happens to it along the way?\n",
    "By the chain rule as usual:\n",
    "\n",
    "$\\displaystyle\\frac{\\partial \\Phi^{l+1}}{\\partial \\Phi^{l}} = W^{l} \\cdot \\frac{\\partial f^{l}}{\\partial \\phi} \\bigg|_{\\phi=\\Phi^{l}}$\n",
    "\n",
    "ie. we take the gradient of the transfer function, evaluated at $\\Phi^{l}$, and push it back through the weights matrix. I've written it all as a vector here: if that's confusing try doing it for a specific element, and then convince yourself that it can be \"vectorised\".\n",
    "\n",
    "\n",
    "So the backward pass through a layer can be written as a $\\Phi^{l} \\longrightarrow \\Phi^{l+1}$ function:\n",
    "\n",
    "$$ \\Phi^{l+1} = W^{l} \\cdot f_{l}(\\Phi^{l}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(X, W, targets, transfer_fns):\n",
    "    \"\"\" Calculates the gradient of every weight in the network\"\"\"\n",
    "    g1, g2, dgood = calc_goodness(X[-1], targets, transfer_fns[-1])\n",
    "    epsilon = dgood\n",
    "    for L in range(len(W), 0, -1):\n",
    "        if L==len(W):\n",
    "            psi = epsilon #it has already been through transfer fn...\n",
    "        if L<len(W):\n",
    "            psi = epsilon * df(X[L], transfer_fns[L]) # elt-wise multiply\n",
    "        n1, n2 = X[L-1].shape[1], psi.shape[1]\n",
    "        A = np.tile(X[L-1],n2).reshape(Npats,n2,n1)\n",
    "        B = np.repeat(psi,n1).reshape(Npats,n2,n1)        \n",
    "        dW[L-1] = (A*B).sum(0) # outer product multiply\n",
    "        epsilon = np.dot(psi, W[L-1]) # inner product multiply\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = forward_pass(X, W, trans)\n",
    "dW = backward_pass(X, W, Targ, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkgrad(dW, X, W, targets, transfer_fns):\n",
    "    # Check the gradient directly, via perturbations to every weight.\n",
    "    # This is completely daft in practical terms, but very useful for debugging.\n",
    "    # ie. it tells you whether your backprop of errors really is returning the true gradient.\n",
    "    tiny = 0.00000001\n",
    "    \n",
    "    dW_test = []\n",
    "    for w in W:\n",
    "        dW_test.append(np.zeros(w.shape, dtype=float))\n",
    "\n",
    "    X = forward_pass(X,W, transfer_fns)\n",
    "    base_good, tmp1, tmp2 = calc_goodness(X[-1], targets, transfer_fns[-1])\n",
    "    \n",
    "    for L in range(0,len(W)):\n",
    "        for j in range(W[L].shape[0]): # index of destination node\n",
    "            for i in range(W[L].shape[1]): # index of origin node\n",
    "                # perturb that weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] + tiny\n",
    "                # compute and store the empirical gradient estimate\n",
    "                X = forward_pass(X, W, transfer_fns)\n",
    "                tmp_good, tmp1, tmp2 = calc_goodness(X[-1], targets, transfer_fns[-1])\n",
    "                (dW_test[L])[j,i] = (tmp_good - base_good)/tiny                \n",
    "                # unperturb the weight\n",
    "                (W[L])[j,i] = (W[L])[j,i] - tiny\n",
    "                \n",
    "    # show the result?\n",
    "    for L in range(0,len(W)):\n",
    "        print ('Layer %d max disagreement: %f'  % (L, ((np.abs(dW[L] - dW_test[L])).ravel().max())))\n",
    "        #print ('-------------- layer %d --------------' %(L))\n",
    "        #print ('calculated gradients:')\n",
    "        #print (dW[L])\n",
    "        #print ('empirical gradients:')\n",
    "        #print (dW_test[L])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkgrad is slow, so I'll try it only for small nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 max disagreement: 0.000112\n",
      "Layer 1 max disagreement: 0.000124\n"
     ]
    }
   ],
   "source": [
    "checkgrad(dW, X, W, Targ, trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yay.\n",
    "The gradient seems to be right for the full MLP, so that's... progress!\n",
    "\n",
    "Let's try learning the problem then...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(X, W, targets, transfer_fns, learning_rate=0.01, momentum=0.1, num_steps=1):\n",
    "    # note dW and prev_change are of the same size as W - we'll make space for them first\n",
    "    times, vals = [], []\n",
    "    next_time = 0\n",
    "    How_Often = 1 # how often to report progress points\n",
    "    \n",
    "    # make space for saving changes to weights, and initialise to zero.\n",
    "    prev_change = []\n",
    "    for L in range(0,len(W)):\n",
    "        prev_change.append(0.0 * np.copy(W[L]))\n",
    "    \n",
    "    # now for the learning iterations\n",
    "    for step in range(num_steps):\n",
    "        X = forward_pass(X, W, transfer_fns)\n",
    "        \n",
    "        # this is just record-keeping.......\n",
    "        if step == next_time:\n",
    "            good_sum, good_vec, dgood = calc_goodness(X[-1], targets, transfer_fns[-1])\n",
    "            vals.append(good_sum)\n",
    "            times.append(step)\n",
    "            next_time = step + How_Often\n",
    "\n",
    "        dW = backward_pass(X, W, targets, transfer_fns)\n",
    "        \n",
    "        for L in range(0,len(W)):\n",
    "            change =  (learning_rate * dW[L]) + (momentum * prev_change[L])\n",
    "            W[L] = W[L] + change\n",
    "            prev_change[L] = change\n",
    "\n",
    "    return W, times, vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new network with random weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are this many neurons in each layer:  [64, 20, 10, 10]\n",
      "Transfer of each processing layer:  ['sigmoid', 'sigmoid', 'softmax']\n",
      "layer 0 activations have shape  (1797, 64)\n",
      "layer 1 activations have shape  (1797, 20)\n",
      "layer 2 activations have shape  (1797, 10)\n",
      "layer 3 activations have shape  (1797, 10)\n",
      "layer 0 weights have shape  (20, 64)\n",
      "layer 1 weights have shape  (10, 20)\n",
      "layer 2 weights have shape  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "arch  = [Nins, 20, 10,  Nouts]\n",
    "trans = ['linear', 'sigmoid', 'sigmoid', 'softmax'] \n",
    "X = initialise_net_structure(inputX, arch, trans)\n",
    "W, dW = initialise_weights(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do it.\n",
    "Run this next cell as often as you want, as learning picks up where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfc08786a0>,\n",
       " <matplotlib.lines.Line2D at 0x7fcfc0878828>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEACAYAAACgS0HpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRhjjGkXSxjGGGPa5f8HJM+/wYEgQu8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfd3871d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, times, vals = learn(X, W, Targ, trans, \n",
    "                       learning_rate=0.0001, \n",
    "                       momentum=0.9, \n",
    "                       num_steps=400)\n",
    "\n",
    "plt.plot(times, vals, 'ob',  times, vals, '-k', alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
